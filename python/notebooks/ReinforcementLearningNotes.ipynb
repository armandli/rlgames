{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning In Tabular Settting\n",
    "* state and action sizes are small\n",
    "* methods:\n",
    "    1. Value Iteration (DP)\n",
    "    2. Policy Iteration (DP)\n",
    "    3. Q-learning\n",
    "    4. SARSA\n",
    "    5. Dyna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning\n",
    "\n",
    "Motivation:\n",
    "    * handle intractably large state and/or action size\n",
    "    * handle continuous state space and/or action space\n",
    "\n",
    "Challenges:\n",
    "    * heavy dependency on the setup of reward\n",
    "    * convergence of approximation function\n",
    "    * non-iid sampling or sampling efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Based vs Model Free vs Hybrid\n",
    "* model based\n",
    "    * model implements environment transition probabilities `P(s, a, s')`, and simulate reward process `R(s, a, s')`\n",
    "    * need to implement a model per environment\n",
    "    * adds model bias during learning\n",
    "    * more sample efficient, faster learning, but lower optimal result\n",
    "* model free\n",
    "    * uses Bellman error/Bellman equation for one forward rollout\n",
    "    * unbiased with one forward rollout\n",
    "    * less sample efficient due to no forward planning\n",
    "    * relies on exploration technique for finding optimal solution\n",
    "    * higher optimal result given more iterations\n",
    "    * generic learning algorithm, environment independent\n",
    "* hybrid\n",
    "    * implements a model, but uses model free algorithms\n",
    "    * obtain higher sample efficiency through environment simulation\n",
    "    * allow model free algorithm to reason about alternative path, imaginative path for faster correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-d8772af55dbb>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-d8772af55dbb>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    - Bellman Equation\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Deep Q-Learning, or why not Deep V-learning?\n",
    "- Bellman Equation\n",
    "```\n",
    "V(s) = max_a(R + gamma * sum_s'(Psas' * V(s'))\n",
    "V(s) = max_a(Q(s, a))\n",
    "Q(s,a) = R + gamma * sum_s'(Psas' * max_a'(Q(s', a')))\n",
    "```\n",
    "* the max function is not easily differentiable, isolating it to a subset of the function allows for gradient methods on approximation algorithms\n",
    "* one forward rollout can avoid considering the transitional probability\n",
    "\n",
    "- The Bellman Error\n",
    "```\n",
    "Q(s, a) = Q(s, a) + alpha(R + gamma * Q(s', a') - Q(s, a))\n",
    "```\n",
    "* uses a functional approximation of the Q function\n",
    "* Bellman error help corrects the value of the functional approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Theory of Non-Convergence of Deep Q-learning\n",
    "Deep Q-learning, without stabilization techniques, does not converge, not in theory, not in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stabilization Techniques\n",
    "1. replay buffer, collecting `<S,A,S',R>` pairs\n",
    "2. target Q network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
